#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
E2Netè®­ç»ƒè„šæœ¬ - ä»…ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
æœ€å°æ”¹åŠ¨ç‰ˆæœ¬ï¼Œåªä¿®æ”¹æŸå¤±å‡½æ•°éƒ¨åˆ†
"""

import os
import sys
import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import argparse
from tqdm import tqdm

# å¯¼å…¥æ•°æ®é›†
import dataset

# å¯¼å…¥æ”¹è¿›çš„æŸå¤±å‡½æ•°
from loss import ImprovedLoss

# å¯¼å…¥E2Net
from E2Net import build_e2net


def train_epoch(model, train_loader, optimizer, criterion, epoch, writer, device='cuda'):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    # å¦‚æœencoderæ˜¯å†»ç»“çš„ï¼Œç¡®ä¿å®ƒä¿æŒevalæ¨¡å¼
    if hasattr(model, 'encoder'):
        model.encoder.eval()
    
    # ç»Ÿè®¡
    total_loss = 0
    loss_components = {
        'dice': 0,
        'bce': 0,
        'iou': 0,
        'edge': 0,
        'aux': 0
    }
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', ncols=120)
    
    for batch_idx, (images, masks) in enumerate(pbar):
        images = images.to(device)
        masks = masks.to(device)
        
        # å‰å‘ä¼ æ’­
        Y_hat, M_coarse = model(images)
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = criterion(Y_hat, M_coarse, masks)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # ç´¯è®¡æŸå¤±
        total_loss += loss_dict['total']
        loss_components['dice'] += loss_dict['dice_main']
        loss_components['bce'] += loss_dict['bce_main']
        loss_components['iou'] += loss_dict['iou_main']
        loss_components['edge'] += loss_dict['edge_main']
        loss_components['aux'] += loss_dict['aux']
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{loss_dict["total"]:.4f}',
            'Dice': f'{loss_dict["dice_main"]:.4f}',
            'IoU': f'{loss_dict["iou_main"]:.4f}'
        })
        
        # # æ¯50ä¸ªbatchè¯¦ç»†è¾“å‡º
        # if batch_idx % 50 == 0:
        #     avg_loss = total_loss / (batch_idx + 1)
        #     print(f'\n  Batch {batch_idx}/{len(train_loader)} | '
        #           f'Loss: {loss_dict["total"]:.4f} | '
        #           f'Dice: {loss_dict["dice_main"]:.4f} | '
        #           f'IoU: {loss_dict["iou_main"]:.4f} | '
        #           f'Edge: {loss_dict["edge_main"]:.4f}')
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    n_batches = len(train_loader)
    avg_loss = total_loss / n_batches
    for key in loss_components:
        loss_components[key] /= n_batches
    
    # TensorBoardè®°å½•
    if writer is not None:
        writer.add_scalar('Train/Loss', avg_loss, epoch)
        writer.add_scalar('Train/Dice', loss_components['dice'], epoch)
        writer.add_scalar('Train/BCE', loss_components['bce'], epoch)
        writer.add_scalar('Train/IoU', loss_components['iou'], epoch)
        writer.add_scalar('Train/Edge', loss_components['edge'], epoch)
        writer.add_scalar('Train/Aux', loss_components['aux'], epoch)
    
    print(f'\nEpoch {epoch} Summary: '
          f'Loss={avg_loss:.4f}, '
          f'Dice={loss_components["dice"]:.4f}, '
          f'IoU={loss_components["iou"]:.4f}, '
          f'Edge={loss_components["edge"]:.4f}')
    
    return avg_loss


def validate(model, val_loader, criterion, epoch, writer, device='cuda'):
    """éªŒè¯"""
    model.eval()
    
    total_loss = 0
    loss_components = {
        'dice': 0,
        'iou': 0,
        'bce': 0
    }
    
    print(f'\nRunning validation...')
    print(f'  Validating on {len(val_loader.dataset)} samples...')
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validation', ncols=100)
        for images, masks in pbar:
            images = images.to(device)
            masks = masks.to(device)
            
            Y_hat, M_coarse = model(images)
            loss, loss_dict = criterion(Y_hat, M_coarse, masks)
            
            total_loss += loss_dict['total']
            loss_components['dice'] += loss_dict['dice_main']
            loss_components['iou'] += loss_dict['iou_main']
            loss_components['bce'] += loss_dict['bce_main']
            
            pbar.set_postfix({
                'Loss': f'{loss_dict["total"]:.4f}',
                'Dice': f'{loss_dict["dice_main"]:.4f}'
            })
    
    # è®¡ç®—å¹³å‡
    n_batches = len(val_loader)
    avg_loss = total_loss / n_batches
    for key in loss_components:
        loss_components[key] /= n_batches
    
    # TensorBoard
    if writer is not None:
        writer.add_scalar('Val/Loss', avg_loss, epoch)
        writer.add_scalar('Val/Dice', loss_components['dice'], epoch)
        writer.add_scalar('Val/IoU', loss_components['iou'], epoch)
        writer.add_scalar('Val/BCE', loss_components['bce'], epoch)
    
    print(f'  âœ“ Validation - Loss: {avg_loss:.4f}, '
          f'Dice: {loss_components["dice"]:.4f}, '
          f'IoU: {loss_components["iou"]:.4f}')
    
    return avg_loss


def main():
    parser = argparse.ArgumentParser(description='E2Net Training with Improved Loss')
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--train_dataset', type=str, default='../dataset/TrainDataset')
    parser.add_argument('--val_dataset', type=str, default='../dataset/TestDataset/CAMO')
    parser.add_argument('--val_interval', type=int, default=5)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--encoder_name', type=str, default='facebook/dinov3-vitb16-pretrain-lvd1689m')
    parser.add_argument('--encoder_pretrained', type=str, default='checkpoint/dinov3-vitb16-pretrain-lvd1689m')
    parser.add_argument('--freeze_encoder', action='store_true', default=True)
    parser.add_argument('--feature_dim', type=int, default=128)
    parser.add_argument('--use_simple_encoder', action='store_true', default=True)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=5e-4)
    
    # æŸå¤±å‡½æ•°å‚æ•°ï¼ˆæ”¹è¿›çš„æƒé‡ï¼‰
    parser.add_argument('--lambda1', type=float, default=2.0,
                       help='Dice loss weight (increased from 1.0)')
    parser.add_argument('--lambda2', type=float, default=1.0,
                       help='BCE loss weight')
    parser.add_argument('--lambda3', type=float, default=0.3,
                       help='Auxiliary loss weight (decreased from 0.5)')
    parser.add_argument('--lambda_edge', type=float, default=0.5,
                       help='Edge loss weight (new)')
    parser.add_argument('--lambda_iou', type=float, default=0.5,
                       help='IoU loss weight (new)')
    
    # ä¿å­˜å‚æ•°
    parser.add_argument('--save_dir', type=str, default='checkpoint/E2Net_ImprovedLoss_v3')
    parser.add_argument('--log_dir', type=str, default='logs/E2Net_ImprovedLoss_v3')
    parser.add_argument('--save_interval', type=int, default=10)
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--device', type=str, default='cuda')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)
    
    # TensorBoard
    writer = SummaryWriter(args.log_dir)
    
    print("="*70)
    print("E2Net Training with Improved Loss Function")
    print("="*70)
    print("\nğŸ”§ Loss Function Configuration:")
    print(f"  Dice weight:  {args.lambda1} (original: 1.0)")
    print(f"  BCE weight:   {args.lambda2}")
    print(f"  IoU weight:   {args.lambda_iou} (new)")
    print(f"  Edge weight:  {args.lambda_edge} (new)")
    print(f"  Aux weight:   {args.lambda3} (original: 0.5)")
    print("="*70)
    
    # æ•°æ®é›†é…ç½®
    train_cfg = dataset.Config(
        datapath=args.train_dataset,
        mode='train',
        batch=args.batch_size,
        lr=args.lr,
        epochs=args.epochs
    )
    
    # åŠ è½½è®­ç»ƒæ•°æ®é›†
    train_data = dataset.Data(train_cfg, 'E2Net')
    train_loader = DataLoader(
        train_data,
        collate_fn=train_data.collate,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    print(f'\nğŸ“Š Training dataset size: {len(train_data)}')
    
    # åŠ è½½éªŒè¯æ•°æ®é›†
    val_loader = None
    if args.val_dataset and args.val_dataset != 'None':
        val_cfg = dataset.Config(
            datapath=args.val_dataset,
            mode='train',
            batch=1
        )
        val_data = dataset.Data(val_cfg, 'E2Net')
        val_loader = DataLoader(
            val_data,
            collate_fn=val_data.collate,
            batch_size=1,
            shuffle=False,
            num_workers=0
        )
        print(f'ğŸ“Š Validation dataset size: {len(val_data)}')
    
    # æ„å»ºæ¨¡å‹
    print(f'\nğŸ—ï¸  Building model...')
    
    model = build_e2net(
        cfg=None,
        encoder_name=args.encoder_name,
        encoder_pretrained=args.encoder_pretrained,
        freeze_encoder=args.freeze_encoder,
        feature_dim=args.feature_dim,
        use_simple_encoder=args.use_simple_encoder
    )
    model = model.to(args.device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'  Total parameters: {total_params/1e6:.2f}M')
    print(f'  Trainable parameters: {trainable_params/1e6:.2f}M')
    
    # æ”¹è¿›çš„æŸå¤±å‡½æ•°
    print(f'\nğŸ’¡ Setting up improved loss function...')
    criterion = ImprovedLoss(
        lambda1=args.lambda1,
        lambda2=args.lambda2,
        lambda3=args.lambda3,
        lambda_edge=args.lambda_edge,
        lambda_iou=args.lambda_iou
    )
    print(f'  âœ“ Using ImprovedLoss with:')
    print(f'    - Dice loss (weight={args.lambda1})')
    print(f'    - BCE loss (weight={args.lambda2})')
    print(f'    - IoU loss (weight={args.lambda_iou}) [NEW]')
    print(f'    - Edge loss (weight={args.lambda_edge}) [NEW]')
    print(f'    - Auxiliary loss (weight={args.lambda3})')
    
    # ä¼˜åŒ–å™¨ï¼ˆä¿æŒåŸæ¥çš„Cosineè°ƒåº¦å™¨ï¼‰
    print(f'\nâš™ï¸  Setting up optimizer and scheduler...')
    optimizer = optim.AdamW(
        [p for p in model.parameters() if p.requires_grad],
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # # Cosineé€€ç«ï¼ˆä¿æŒåŸæ ·ï¼‰
    # scheduler = optim.lr_scheduler.CosineAnnealingLR(
    #     optimizer,
    #     T_max=args.epochs,
    #     eta_min=1e-6
    # )
    # å½“å‰ä½¿ç”¨ CosineAnnealingLRï¼Œè€ƒè™‘æ”¹ä¸º ReduceLROnPlateau
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',
        factor=0.5,        # é™ä½50%
        patience=10,       # 10ä¸ªepochæ— æ”¹å–„åˆ™é™ä½
        min_lr=1e-6,
        verbose=True
    )
    
    print(f'  âœ“ Optimizer: AdamW (lr={args.lr}, weight_decay={args.weight_decay})')
    print(f'  âœ“ Scheduler: CosineAnnealingLR (T_max={args.epochs})')
    
    # è®­ç»ƒé…ç½®
    print("\n" + "="*70)
    print("Training Configuration")
    print("="*70)
    print(f"  Epochs:         {args.epochs}")
    print(f"  Batch size:     {args.batch_size}")
    print(f"  Learning rate:  {args.lr}")
    print(f"  Val interval:   {args.val_interval}")
    print(f"  Save interval:  {args.save_interval}")
    print("="*70)
    
    # å¼€å§‹è®­ç»ƒ
    print(f'\nğŸš€ Starting training...\n')
    
    best_val_loss = float('inf')
    
    for epoch in range(1, args.epochs + 1):
        print(f'\n{"="*70}')
        print(f'Epoch {epoch}/{args.epochs}')
        print(f'{"="*70}')
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, optimizer, criterion,
                                epoch, writer, args.device)
        
        # éªŒè¯
        if val_loader is not None and epoch % args.val_interval == 0:
            val_loss = validate(model, val_loader, criterion, epoch, writer, args.device)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_path = os.path.join(args.save_dir, 'E2Net_best.pth')
                torch.save(model.state_dict(), best_path)
                print(f'\nâœ“ Saved best model: {best_path} (Val Loss: {val_loss:.4f})')
                
        # æ›´æ–°å­¦ä¹ ç‡
        metric_for_scheduler = val_loss if val_loss is not None else train_loss
        scheduler.step(metric_for_scheduler)
        # scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        print(f'Current learning rate: {current_lr:.6f}')
        
        # å®šæœŸä¿å­˜
        if epoch % args.save_interval == 0:
            checkpoint_path = os.path.join(args.save_dir, f'E2Net_epoch_{epoch}.pth')
            torch.save(model.state_dict(), checkpoint_path)
            print(f'Saved checkpoint: {checkpoint_path}')
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(args.save_dir, 'E2Net_final.pth')
    torch.save(model.state_dict(), final_path)
    
    print('\n' + "="*70)
    print('âœ“ Training completed!')
    print("="*70)
    print(f'  Best model: {best_path} (Val Loss: {best_val_loss:.4f})')
    print(f'  Final model: {final_path}')
    print("="*70)
    
    writer.close()


if __name__ == '__main__':
    main()